Android Framewokd Audio Study
https://blog.csdn.net/qq_24451593/article/details/80325908
https://blog.csdn.net/zyuanyun/article/details/60890534
https://www.cnblogs.com/yangjies145/p/6883705.html
https://developer.android.google.cn/guide/topics/media/volumeshaper.html?hl=zh-cn(volumeshaper音量控制)
Android Audio Architecture
https://www.jianshu.com/p/a676d4d959ae
Android AudioManager详解
https://blog.csdn.net/h183288132/article/details/51151966
#====================================================================
基本概念详解:
帧(frame)的概念:帧表示一个完整的声音单元,所谓的声音单元是指一个采样样本;
如果是双声道,那么一个完整的声音单元就是2个样本,如果是5.1声道,那么一个完整的声音单元就是6个样本了.;帧的大小(一个完整的声音单元的数据量)等于声道数乘以采样深度,即frameSize = channelCount * bytesPerSample.帧的概念非常重要,无论是框架层还是内核层,都是以帧为单位去管理音频数据缓冲区的.
其次还得了解传输延迟(latency)的概念:传输延迟表示一个周期的音频数据的传输时间.我们再次引入周期(period)的概念:
Linux ALSA把数据缓冲区划分为若干个快,dma每传输完一个块上的数据即发出一个硬件中断,cpu收到中断信号后,在配置dma去传输下一个块上的数据;一个块即一个周期,周期大小(perodSize)即是一个数据块的帧数.
在回到传输延迟(latency),传输延迟等于周期大小除以采样率,即latency = periodSize/sampleRate.
音频重采样:把一个采样率的数据转换为另一个采样率的数据.Android原生系统上,音频硬件设备一般都工作在一个固定的采样率上(如48Hz),因此所有音轨数据都需要重采样

重采样计算:afFrameCount是硬件设备处理单个数据块的帧数,afSampleRate是硬件设备配置的采样率,sampleRate是音轨的采样率.如果把音轨数据重采样到afSampleRate上,那么反推算出应用程序最少传入的帧数为为afFrameCount*sampleRate/afSampleRate,而为了播放流畅,实际上还要大一点,所以在乘以一个系数(可参考framebuffer双缓冲,一个缓冲缓存当前的图像,一个缓冲准备下一副的图像,这样图像切换更流畅,然后就得出一个可以保证播放流畅的最低帧数)
minFrameCount = (afFameCount*sampleRate/afSampleRate)*minBufCount.
#====================================================================
sampleRate采样率:音频的采样频率,每秒钟能够采样的次数,采样率越高,音质越高.给出的实例是44100,22050,11020但不限于这几个参数.例如要采集低质量的音频就可以使用4000,8000等低采样率.
channelConfig声道设置:android支持双声道立体音和单声道.MONO单声道,STEREO立体声.补充:指声音在录制或播放时在不同空间位置采集或回放的相互独立的音频信号,所以声道数也就是声音录制时的音源数量或回放时相应的扬声器数量.
audioFormat编码制式和采样大小:采集来的数据当然使用PCM编码(脉冲代码调制编码,即PCM编码.PCM通过采样,量化,编码三个步骤将连续变化的模拟信号转换为数字编码.)android支持的采样大小16bit或者8bit.当然采样大小越大,那么信息量越多,音质也越高,现在主流的采样大小都是16bit,在低质量的语音传输的时候8bit足够了.补充:采集卡(声卡)的位.客观地反映了数字信号对输入声音凶耗描述的准确程度.8位的采集卡,可以将声音分为256(2的8次方)个精度单位处理;16位的采集卡,可以将声音分为64K(2的16次方)个精度单位处理
bufferSizeInBytes:采集数据需要的缓冲区的大小,如果不知道最小需要的大小可以在getMinBufferSize()查看
//设置线程的优先级,优先考虑最重要的音频线程.
android.os.Process
    .setThreadPriority(android.Process.THREAD_PRIORITY_URGENT_AUDIO)

#====================================================================
一. 综述
Audio系统是Android平台的重要组成部分,它主要包括三方面的内容:
1. AudioRecorder和AudioTrack：这两个类属于Audio系统对外提供的API类，通过它们可以完成Android平台上音频数据的采集和输出任务.

2. AudioFlinger：它是Audio系统的工作引擎，管理着系统中的输入输出音频流，并承担音频数据的混音，以及读写Audio硬件以实现数据的输入输出等工作.

3. AudioPolicyService，它是Audio系统的策略控制中心，具有掌管系统中声音设备的选择和切换、音量控制等功能.

#====================================================================
二. AudioTrack的破解
AudioTrack属于Audio系统对外提供的API类，所以它在Java层和Native层均有对应类，先从Java层的用例开始.

#====================================================================
2.1 [AudioTrackAPI使用例子（Java层）]
① 根据音频数据的特性来确定所要分配的缓冲区的最小size
int bufsize = AudioTrack.getMinBufferSize(8000,//采样率：每秒8K个点 
                    AudioFormat.CHANNEL_CONFIGURATION_STEREO,//声道数：双声道      
                    AudioFormat.ENCODING_PCM_16BIT//采样精度：一个采样点16比特，相当于2个字节
                    );
//按照数字音频的知识这个算出来的是一秒钟buffer大小

② 创建AudioTrack
AudioTrack trackplayer = new AudioTrack(
                    AudioManager.STREAM_MUSIC,//音频流类型
                    8000,//采样率
                    AudioFormat.CHANNEL_CONFIGURATION_STEREO,//双声道
                    AudioFormat.ENCODING_PCM_16BIT,//采样精度:一个采样点16比特,相当于2个字节
                    bufsize,一秒钟buffer的大小
                    AudioTrack.MODE_STREAM//数据加载模式
                    );

③ 开始播放
trackplayer.play();

④ 调用write写数据
trackplayer.write(bytes_pkg, 0,bytes_pkg.length);//往track中写数据

⑤ 停止播放和释放资源
trackplayer.stop();//停止播放
trackplayer.release();//释放底层资源

#====================================================================
2.2 AudioTrack的数据加载模式
AudioTrack有两种数据加载模式：MODE_STREAM和MODE_STATIC，它们对应着两种完全不同的使用场景。
(1) MODE_STREAM：在这种模式下，通过write一次次把音频数据写到AudioTrack中。这和平时通过write系统调用往文件中写数据类似，但这种工作方式每次都需要把数据从用户提供的Buffer中拷贝到AudioTrack内部的Buffer中，这在一定程度上会使引入延时。为解决这一问题，AudioTrack就引入了第二种模式。

(2) MODE_STATIC：这种模式下，在play之前只需要把所有数据通过一次write调用传递到AudioTrack中的内部缓冲区，后续就不必再传递数据了。这种模式适用于像铃声这种内存占用量较小，延时要求较高的文件。但它也有一个缺点，就是一次write的数据不能太多，否则系统无法分配足够的内存来存储全部数据。

注意：如果采用STATIC模式，须先调用write写数据，然后再调用play

#====================================================================
2.3 音频流的类型
在AudioTrack构造函数中，会接触到AudioManager.STREAM_MUSIC这个参数。它的含义与Android系统对音频流的管理和分类有关。
Android将系统的声音分为好几种流类型，下面是几个常见的：
    · STREAM_ALARM：警告声
    · STREAM_MUSIC：音乐声，例如music等
    · STREAM_RING：铃声
    · STREAM_SYSTEM：系统声音，例如低电提示音，锁屏音等
    · STREAM_VOCIE_CALL：通话声

注意：上面这些类型的划分和音频数据本身并没有关系。例如MUSIC和RING类型都可以是某首MP3歌曲。另外，声音流类型的选择没有固定的标准，例如，铃声预览中的铃声可以设置为MUSIC类型。音频流类型的划分和Audio系统对音频的管理策略有关。

#====================================================================
2.4 Buffer分配和Frame的概念
在用例中碰到的第一个重要函数就是getMinBufferSize。这个函数对于确定应用层分配多大的数据Buffer具有重要指导意义

#====================================================================
2.5 AudioTrack(Java空间)的分析
java端创建一个AudioTrack实例,native端也会创建一个native AudioTrack实例
分析代码
(1)校验传入参数
(2)校验成功后会调用native_setup()将参数向下传递

#====================================================================
2.6 native_setup()函数分析(JNI层)
(1)获取参数sessionId
(2)判断是否存在native层AudioTrack对象如果不存在则创建对象,
   判断传入参数是否正确,
   创建native AudioTrack对象
   将AudioAttibutes参数的数值(usage,contenttype,flags,tags)出传入到audio_attributes_t对象中
(2.1)创建AudioTrackJniStorage对象,可以通过此对象来回调到java端
   将java端的AudioTrack实例对象和它的弱引用保存到AudioTrackJniStorage成员变量中
(2.2)初始化native AudioTrack对象,调用AudioTrack.set()
   如果是MODE_STREAM则不需要创建共享内存
   如果是MODE_STATIC先创建匿名共享内存,并传入AudioTrack.set()接口中
   注:Android系统匿名空想内存需要查看MemoryHeapBase和MemoryBase这两个类
(3)如果已经存在native AudioTrack,则重新创建AudioTrackJniStorage对象并重新将java端AudioTrack对象传入AudioTrackJniStorage对象变量中
(4)再次检验参数是否正确,并将一些参数设置回java端AudioTrack对象
(4.1)将native AudioTrack对象传递给java端mNativeTrackInJavaObj变量
(4.2)将AudioTrackJniStorage对象传递给java端mJniData变量
(4.3)将native AudioTrack.streamType()返回值传递给java端mStreamType变量
(5)如果初始化失败销毁对象并将参数置空

#====================================================================
2.6 AudioTrackJniStorage分析
AudioTrackJniStorage是一个辅助类，其中有一些有关共享内存方面的较重要的知识
(1) 共享内存介绍
共享内存，作为进程间数据传递的一种手段，在AudioTrack和AudioFlinger中被大量使用。先简单了解一下有关共享内存的知识：

·  每个进程的内存空间是4GB，这个4GB是由指针长度决定的，如果指针长度为32位，那么地址的最大编号就是0xFFFFFFFF，为4GB。

·  上面说的内存空间是进程的虚拟地址空间。换言之，在应用程序中使用的指针其实是指向虚拟空间地址的。那么，如何通过这个虚地址找到存储在真实物理内存中的数据呢？

上面的问题，引出了内存映射的概念。内存映射让虚拟空间中的内存地址和真实物理内存地址之间建立了一种对应关系。也就是说，进程中操作的0x12345678这块内存的地址，在经过OS内存管理机制的转换后，它实际对应的物理地址可能会是0x87654321。当然，这一切对进程来说都是透明的，这些活都由操作系统悄悄地完成了。这和我们的共享内存会有什么关系吗？

如何创建和共享内存呢？不同系统会有不同的方法。Linux平台的一般做法是：

·  进程A创建并打开一个文件，得到一个文件描述符fd。

·  通过mmap调用将fd映射成内存映射文件。在mmap调用中指定特定参数表示要创建进程间共享内存。

·  进程B打开同一个文件，也得到一个文件描述符，这样A和B就打开了同一个文件。

·  进程B也要用mmap调用指定参数表示想使用共享内存，并传递打开的fd。这样A和B就通过打开同一个文件并构造内存映射，实现了进程间内存共享。

注意，这个文件也可以是设备文件。一般来说，mmap函数的具体工作由参数中的那个文件描述符所对应的驱动或内核模块来完成。

除上述一般方法外，Linux还有System V的共享内存创建方法，这里就不再介绍了。总之，AT和AF之间的数据传递，就是通过共享内存方式来完成的。这种方式对于跨进程的大数据量传输来说，是非常高效的。

(2) MemoryHeapBase和MemoryBase类介绍
AudioTrackJniStorage用到了Android对共享内存机制的封装类。
总结:
    (2.1)分配了一块共享内存，这样两个进程可以共享这块内存。

    (2.2)基于Binder通信，这样使用这两个类的进程就可以交互了。

#====================================================================
2.7 AudioTrack(java端) Play()分析
(1) 判断是否延迟播放,如果不是延迟调用native_start()
    如果延迟播放则另起线程进行等待延迟时间后进行播放
(2) jni层调用native AudioTrack.start();

#====================================================================
2.8 AudioTrack(java端) write()分析
(1) Java层的write函数有两个
    ·  一个是用来写PCM16数据的，它对应的一个采样点的数据量是两个字节。
    ·  另外一个用来写PCM8数据的，它对应的一个采样点的数据量是一个字节
    我们分析PCM16数据的调用
(2) 判断参数及状态是否正常
(3) 调用native_write_short(),返回读取数据字节数

#====================================================================
2.8.1 AudioTrack(jni) android_media_AudioTrack_writeArray分析
(1)参数校验,及获取传入数组指针
(2)调用writeToTrack()
    (2.1)如果是STREAM模式,sharedBuffer()返回空,则调用Native Track的write函数
    (2.2)如果是STATIC模式，sharedBuffer()返回不为空,则将java端传入的数据通过memcpy拷贝到共享内存
注:在STATIC模式下，直接把数据memcpy到共享内存，记住在这种模式下要先调用write,后调用play

#====================================================================
2.7 AudioTrack(java端) release()分析
当数据都write完后，需要调用stop停止播放，或者直接调用release来释放相关资源。由于release和stop有一定的相关性，这里只分析release调用.
(1) 先调用stop
(2) 调用native_release()

#====================================================================
2.7.1 AudioTrack(jni) android_media_AudioTrack_release
(1) 将java端AudioTrack对象中的native参数置空
(2) 删除AudioTrackJniStorage对象
android_media_AudioTrack_stop分析
(1) 调用native AudioTrack.stop();

#====================================================================
2.8 AudioTrack(Java空间)的分析总结
(1) new一个AudioTrack,使用无参构造
(2) 调用set函数,把java层的参数传进去,另外还设置了一个audiocallback回调函数
(3) 调用了AudioTrack的start函数
(4) 调用了AudioTrack的write函数
(5) 工作完毕后,调用stop
(6) 最后就是Native对象的delete

#====================================================================
三 AudioTrack(Native空间)的分析
3.1 AudioTrack和set分析
status_t AudioTrack::set(
        audio_stream_type_t streamType,//音频类型
        uint32_t sampleRate,//采样率
        audio_format_t format,//采样精度:一个采样点16比特,相当于2个字节
        audio_channel_mask_t channelMask,
        size_t frameCount,//由计算得来
        audio_output_flags_t flags,
        callback_t cbf,//audiocallback
        void* user,//java端AudioTrack全局引用
        int32_t notificationFrames,//0
        const sp<IMemory>& sharedBuffer,//共享内存
        bool threadCanCallJava,//线程是否能回调回java端
        audio_session_t sessionId,//audio session id
        transfer_type transferType,//同步写
        const audio_offload_info_t *offloadInfo,
        uid_t uid,//-1
        pid_t pid,//-1
        const audio_attributes_t* pAttributes,//java端AudioAttribute对象属性传递到audio_attributes_t对象中
        bool doNotReconnect,//false
        float maxRequiredSpeed,//1.0f
        audio_port_handle_t selectedDeviceId//AUDIO_PORT_HANDLER_NONE
        )
(1)设置参数给本地变量
(2)校验参数是否正确
(3)创建AudioTrackThread线程通知回java端
(4)创建IAudioTrack与AudioFlinger进程交互,调用createTrack_l()函数

3.2 createTrack_l()分析
(1)获取与AudioFlinger进程交互的binder对象
(2)将相关参数复制给IAudioFlinger::CreateTrackInput input;对象
(3)调用createTrack()将input传递过去,放回IAudioTrack binder对象
(4)此时AudiFlinger
(5)在STREAM模式下，没有在AT端创建共享内存，但前面提到了AudioTrack和AudioFlinger的数据交互是通过共享内存完成的，这块共享内存最终由AudioFlinger的createTrack创建.
(6)IMemory的pointer()在此处将返回共享内存的首地址，类型为void*,static_cast直接把这个void*类型转成audio_track_cblk_t，表明这块内存的首部中存在audio_track_cblk_t这个对象
(7)audio_track_cblk_t.out = 1;//out为1表示输出，out为0表示输入
(8)audio_track_cblk_t.buffers.buffers指向数据空间，它的起始位置是共享内存的首部加上audio_track_cblk_t的大小
(9)创建AudioTrackClientProxy,保存audio_track_cblk_t对象,属于这个对象客户端的代理类

#====================================================================
3.2 IAudioTrack和AudioTrack和AudioFlinger的关系
通过前面的代码分析,我们发现IAudioTrack中有一块共享内存,其头部是一个audio_track_cblk_t(简称CB)对象,在该对象之后才是数据缓冲,这个CB对象有什么作用呢?

#====================================================================
3.3 共享内存及其Control Block ,audio_track_cblk_t分析
MemoryHeapBase和MemoryBase都没有提供同步对象,那么,AudioTrack和AudioFlinger作为典型的数据生产者和消费者,如何正确协调二者生产和消费的步调呢?
Android为顺应民意，便创造出了这个CB对象，其主要目的就是协调和管理AT和AF二者数据生产和消费的步伐。先来看CB都管理些什么内容。它的声明在AudioTrackShared.h中，而定义却在AudioTrack.cpp中。
struct audio_track_cblk_t
{
    Condition   cv;//这是两个同步变量，初始化的时候会设置为支持跨进程共享

    /*一块数据缓冲同时被生产者和消费者使用，最重要的就是维护它的读写位置了。
    下面定义的这些变量就和读写的位置有关，虽然它们的名字并不是那么直观。
    另外，这里提一个扩展问题，读者可以思考一下：
    volatile支持跨进程吗？要回答这个问题需要理解volatile、CPU Cache机制和共享 内存的本质*/
    volatile    uint32_t    user;   //当前写位置（即生产者已经写到什么位置 了）
    volatile    uint32_t   server;  //当前读位置
    void*       buffers; //指向数据缓冲的首地址
    uint32_t    frameCount;//数据缓冲的总大小，以Frame为单位
    uint32_t    loopStart; //设置打点播放（即设置播放的起点和终点）
    uint32_t    loopEnd;
    int         loopCount;//循环播放的次数
    uint8_t     flowControlFlag;//控制标志，见下文分析
    uint8_t     out; // AudioTrack为1，AudioRecord为0

    volatile    union {
                    uint16_t    volume[2];
                    uint32_t    volumeLR;

                };//联合体,和音量有关系，可以不管它

    uint32_t    stepUser(uint32_tframeCount);//更新写位置
    bool        stepServer(uint32_tframeCount);//更新读位置
    void*       buffer(uint32_toffset) const;//返回可写空间起始位置
    uint32_t    framesAvailable();//还剩多少空间可写
    uint32_t    framesReady();//是否有可读数据

·  对于音频输出来说，flowControlFlag对应着under run状态，under run状态是指生产者提供数据的速度跟不上消费者使用数据的速度。这里的消费者指的是音频输出设备。由于音频输出设备采用环形缓冲方式管理，当生产者没有及时提供新数据时，输出设备就会循环使用缓冲中的数据，这样就会听到一段重复的声音。这种现象一般被称作“machinegun”。对于这种情况，一般的处理方法是暂停输出，等数据准备好后再恢复输出。

·  对于音频输入来说，flowControlFlag对于着overrun状态，它的意思和underrun一样，只是这里的生产者变成了音频输入设备，而消费者变成了Audio系统的AudioRecord。

关于audio_track_cblk_t还有一个神秘的问题
mCblk =static_cast<audio_track_cblk_t*>(cblk->pointer());
·  cblk->pointer返回的是共享内存的首地址，怎么把audio_track_cblk_t对象塞到这块内存中呢？
这个问题将通过对AudioFlinger的分析，得到答案。

#====================================================================
3.4 数据的Push or Pull
在JNI层的代码中可以发现，在构造AudioTrack时，传入了一个回调函数audioCallback。由于它的存在，导致了Native的AudioTrack将创建另一个线程AudioTrackThread。它有什么用呢？
这个线程与外界数据的输入方式有关系，AudioTrack支持两种数据输入方式：
·  Push方式：用户主动调用write写数据，这相当于数据被push到AudioTrack。MediaPlayerService一般使用这种这方式提供数据。
·  Pull方式：AudioTrackThread将利用这个回调函数，以EVENT_MORE_DATA为参数主动从用户那pull数据。ToneGenerator使用这种方式为AudioTrack提供数据。
这两种方式都可以使用，不过回调函数除了EVENT_MORE_DATA外，还能表达其他许多意图，这是通过回调函数的第一个参数来表明的。一起来看：
[-->AudioTrack.h::event_type]

enum event_type {
    EVENT_MORE_DATA = 0, //表示AudioTrack需要更多数据
    EVENT_UNDERRUN = 1,//这是Audio的一个术语，表示Audio硬件处于低负荷状态
    //AT可以设置打点播放，即设置播放的起点和终点,LOOP_END表示已经到达播放终点
    EVENT_LOOP_END= 2,
    /*
      数据使用警戒通知。该值可通过setMarkerPosition ()设置。
      当数据使用超过这个值时，AT会且仅通知一次，有点像WaterMarker。
      这里所说的数据使用，是针对消费者AF消费的数据量而言的
    */
    EVENT_MARKER = 3,
    /*
      数据使用进度通知。进度通知值由setPositionUpdatePeriod()设置，
      例如每使用500帧通知一次
    */
       EVENT_NEW_POS = 4,
       EVENT_BUFFER_END = 5   //数据全部被消耗
    };


#====================================================================
3.5 请看AudioTrackThread的线程函数threadLoop
(1)调用创建线程的AudioTrack的processAudioBuffer函数
    (1.1)processAudioBuffer分析
    (1.2)status_t err = obtainBuffer(&audioBuffer, 1);//得到一块可写的缓冲
    (1.3)mCbf(EVENT_MORE_DATA, mUserData, &audioBuffer);//从用户那pull数据
    (1.4)releaseBuffer(&audioBuffer);//读写完毕释放缓冲区
     用例会调用write函数写数据，AudioTrackThread的回调函数也让我们提供数据。难道我们同时在使用Push和Pull模式？这太奇怪了！来查看这个回调函数的实现，了解一下究竟是怎么回事。该回调函数是通过set调用传入的，对应的函数是audioCallback。audioCallback中并没有进行pull模式

#====================================================================
3.6 Native 层AuioTrack write输入数据
write函数涉及Audio系统中最重要的关于数据如何传输的问题，在分析它的时候，不妨先思考一下它会怎么做。回顾一下我们已了解的信息：
·  有一块共享内存.
·  有一个控制结构，里边有一些支持跨进程的同步变量.
有了这些东西，write的工作方式就非常简单了：
·  通过共享内存传递数据
·  通过控制结构协调生产者和消费者的步调
(1)Buffer audioBuffer;是一个辅助性结构
(2)audioBuffer.frameCount = userSize/frameSize();//以帧为单位
(3)status_terr = obtainBuffer(&audioBuffer, -1);//obtainBuffer从共享内存中得到一块空闲的数据块
(4)memcpy(audioBuffer.i8, buffer, toWrite);//地址在audioBuffer.i8中，数据传递通过memcpy完成
(5)releaseBuffer(&audioBuffer);//releaseBuffer更新写位置，同时会触发消费者

#====================================================================
3.7 obtainBuffer和releaseBuffer分析
这两个函数展示了做为声场这AudioTrack和audio_track_cblk_t对象的交互方法
(1)obtainBuffer的功能，就是从audio_track_cblk_t管理的数据缓冲中得到一块可写空间，
(2)releaseBuffer，则是在使用完这块空间后更新写指针的位置

#====================================================================
3.8 delete AudioTrack分析
AudioTrack::~AudioTrack()
(1)调用stop()函数,
   (1.1)判断isOffloaded_l当前状态是否为退出状态
   (1.2)调用调用内存代理对象AudioTrackClientProxy.stop()
   (1.3)调用IAudioTrack.stop()
   (1.4)使AudioTrackThread线程退出
(2)停止AudioTrackThread线程
(3)调用IAudioTrack.clear(),调用到AudioFlinger侧的clear()函数
(4)清空如果是static模式传递过来的共享内存
stop的工作比较简单，就是调用IAudioTrack的stop，并且还要求退出回调线程。要重点关注IAudioTrack的stop函数，这个将做为AT和AF交互流程中的一个步骤来分析

#====================================================================
四.AudioFlinger的破解
AudioFlinger是Audio系统的核心,来自AudioTrack的数据,最终在这里得到处理,并被写入AudioHAL层

#====================================================================
4.1 AudioFlinger的诞生
AudioFlinger及AudioPolicyService都由audioserver进程中创建并启动(main_audioserver.cpp)
AudioFlinger::instantiate();
AudioPolicyService::instantiate();

AudioFlinger::AudioFlinger()
    : BnAudioFlinger(),
      mMediaLogNotifier(new AudioFlinger::MediaLogNotifier()),
      mPrimaryHardwareDev(NULL),//代表Audio硬件的HAL对象,为mAudioHwDevs数组的首元素或者是空
      mAudioHwDevs(NULL),//代表Audio硬件的HAL对象数组
      mHardwareStatus(AUDIO_HW_IDLE),//硬件当前状态,只是为了dump命令使用
      mMasterVolume(1.0f),
      mMasterMute(false),
      // mNextUniqueId(AUDIO_UNIQUE_ID_USE_MAX),
      mMode(AUDIO_MODE_INVALID),
      mBtNrecIsOff(false),
      mIsLowRamDevice(true),//是否是低内存设备
      mIsDeviceTypeKnown(false),//当前设备是否是已知的
      mTotalMemory(0),//总空间大小
      mClientSharedHeapSize(kMinimumClientSharedHeapSizeBytes),
      mGlobalEffectEnableTime(0),
      mSystemReady(false)//系统是否已经准备完成
(1)设置mNextUniqueIds数组中元素为AUDIO_UNIQUE_ID_USE_MAX
(2)工厂模式创建
(2.1)DevicesFactoryHalInterface::create();
(2.2)EffectsFactoryHalInterface::create();




#====================================================================
4.2 DeviceHalInterface介绍
DeviceHalInterface是Android对代表Audio硬件的封装,属于HAL层.HAL层的具体功能,由各个硬件厂商根据所选硬件的情况来实现,多以动态库的形式提供.这里简单分析一下
class DeviceHalInterface : {
    //用于检查硬件是否初始化成功，返回的错误码定义在include/utils/Errors.h
    virtual status_t initCheck() = 0;
    //设置通话音量,范围从0到1.0
    virtual status_t setVoiceVolume(float volume) = 0;
    //设置除通话音量外的其他所有音频流类型的音量，范围从0到1.0，如果硬件不支持的话，这个功能会由软件层的混音器完成
    virtual status_t setMasterVolume(float volume) = 0;
    //设置模式，NORMAL的状态为普通模式，RINGTONE表示来电模式（这时听到的声音是来电铃声）,IN_CALL表示通话模式（这时听到的声音是手机通话过程中的语音）
    virtual status_t setMode(intmode) = 0;
    // 和麦克相关
    virtual status_t setMicMute(bool state) = 0;
    virtual status_t getMicMute(bool* state) = 0;
    // 设置/获取配置参数，采用key/value的组织方式
    virtual status_t setParameters(const String8& keyValuePairs) = 0;
    virtual String8 getParameters(const String8& keys) = 0;
    // 根据传入的参数得到输入缓冲的大小，返回0表示其中某个参数的值Audio HAL不支持
    virtualsize_t getInputBufferSize(uint32_tsampleRate, int format   ,int channelCount) = 0;
    /*下面这几个函数非常重要 */
    /*
        openOutputStream：创建音频输出流对象（相当于打开音频输出设备）
        AF可以往其中write数据，指针型参数将返回该音频输出流支持的类型、声道数、采样率等
    */
    virtual status_t openOutputStream(
        audio_io_handle_t handle,
        audio_devices_t devices,
        audio_output_flags_t flags,
        struct audio_config *config,
        const char *address,
        sp<StreamOutHalInterface> *outStream) = 0;
    //关闭音频输出流
    virtual void closeOutputStream(AudioStreamOut* out) = 0;

    /* 创建音频输入流对象（相当于打开音频输入设备），AF可以read数据*/
    virtual status_t openInputStream(
            audio_io_handle_t handle,
            audio_devices_t devices,
            struct audio_config *config,
            audio_input_flags_t flags,
            const char *address,
            audio_source_t source,
            sp<StreamInHalInterface> *inStream) = 0;
    //关闭音频输入流
    virtual void closeInputStream(AudioStreamIn* in) =0;
}

DeviceHalInterface管理音频输出设备对象(AudioStreamOut),和音频输入设备对象(AudioStreamIn)的创建

音频输出/输入对象均支持设置参数(由setParameters完成)
说明:DeviceHalInterface最终要的功能是创建AudioStreamOut和AudioStreamIn,他们分别代表音频输出设备和输入设备.从这个角度说,DeviceHalInterface管理着系统中所有的音频设备.Android引入HAL层,大大简化了应用层的工作,否则不管是使用libasound(ALSA提供的用户空间库)还是ioctl来控制音频设备,都会非常麻烦

#====================================================================
4.3 通过流程分析AudioFlinger
回顾一下AT和AF的交互流程
(1)AT调用createTrack,得到一个IAudioTrack对象
(2)AT调用IAudioTrack对象的start,表示准备写数据了
(3)AT通过write写数据,这个过程和audio_track_cblk_t有着密切关系
(4)最后AT调用IAudioTrack的stop或delete IAudioTrack结束工作

#====================================================================
4.3.1 createTrack的分析
(1)传参由变为整合后的对象
    1)const CreateTrackInput& input,//入参包含上层传入的所有参数
    2)CreateTrackOutput& output,//出参,由内部修改后传出到外部
    3)status_t *status
(2)对入参进行校验及赋值到本地成员变量
(3)对出参进行赋值,大部分是通过AudioSystem(相当与一个工具类)
(4)PlaybackThread *thread = checkPlayebackThread_l(output.outputId)
//outputId代表索引号,这里根绝索引号找到一个工作线程,它是一个PlaybackThread
(5)client = registerPid(clientPid);
//看看这个进程是否已经是AF的client,AF根据进程pid来标识不同的Client
(6)检查是否有其他线程上存在着这个outputId的客户端,如果有则移除它
(7)rack = thread->createTrack_l(...);
//在找到的工作线程对象中创建一个Track,注意它的类型是Track
    (7.1)createTrack_l分析
(8)trackHandle = new TrackHandle(track);
TrackHandle是Track对象的Proxy,它支持Binder通信,而Track不支持Binder,TrackHandle所接收的请求最终会由Track处理,这是典型的Proxy模式

#====================================================================
4.3.2 Track创建共享内存和TrackHandle
(1)在Track的基类TrackBase的构造函数中创建
//根据size创建一个块共享内存
mCblkMemory = client->heap()->allocate(size);
(2)这里需要重点讲解下下面这句话的意思
new(mCblk)audio_track_cblk_t();
注意它的用法,new后面的括号里是内存,紧接其后的是一个类的构造函数
重点说明:这个语句就是C++语言中的placement new.其含义是在括号里指定的内存中创建一个对象.我们知道,普通的new只能在堆上创建对象,堆的地址由系统分配.这里采用placementnew将使得audio_track_cblk_t创建在共享内存上,它就自然而然的能被多个进程看见并使用.关于placementnew较详细的知识.
通过上面的分析,可以知道:
(1)Track创建了共享内存
(2)CB对象通过placement new方法创建于这块共享内存中
Android在这里使用了Proxy模式,即TrackHandl是Track的代理
(1)Track没有基于Binder通信,它不能接收来自远端进程的请求
(2)TrackHandler能基于Binder通信,它可以接收来自远端进程的请求,并且能调用Track对应的函数

#====================================================================
4.3.3 AudioFlinger中内部类Client对象
Client是AudioFLinger对客户端的封装,凡是使用了AudioTrack和AudioRecord的进程,都会被当做是AF的Client,并且Client用它的进程pid作为标识
注意:一个Client进程可以创建多个AudioTrack,这些AudioTrack都属于一个Client

#====================================================================
4.3.4 工作线程介绍
AudioFlinger中有几种不同类型的工作线程
ThreadBase           ---继承--->   |Thread
---------------------------------------------------
PlayBackThread       ---继承--->   |
                                   |ThreadBase   
RecordThread         ---继承--->   |
---------------------------------------------------
DirectOutputThread   ---继承--->    |
                                    |PlayBackThread
MixerThread          ---继承--->    |
---------------------------------------------------
DuplicatingThread    ---继承--->    |MixThread

(1)PlaybackThread:回放线程,用于音频输出.它有一个成员变量mOutput为AudioStreamOutput*类型,这表明PlaybackThread直接和Audio银屏输出设备建立了联系
(2)RecordThread:录音线程,用于音频输入,它的关系比较单纯,它有一个成员变量mInput为AudioStreamInput*类型,这表明RecordThread直接和Audio音频输入设备建立了联系
从PlaybackThread的派生关系可看出,手机上的音频回放应该比较复杂,否则也不会派生出三个子类.其中:
(1)MixerThread:混音线程,它将来自多个源的音频数据婚姻后再输出
(2)DirectOutputThread:直接输出线程,它会选择一路音频流后将数据直接输出,由于没有混音的操作,这样可以减少很多延时
(3)DuplicatingThread: 多路输出线程,它从MixerThread派生,意味着它也能够混音.它最终会把混音后的数据写到多个输出中,也就是一份数据会有多个接收者,也就是一份数据会有多个接收者.这就是Duplicate的含义,目前在蓝牙A2DP设备输出中使用
补充:
(1)PlaybackThread维护两个Track数组,一个是mActiveTracks,表示当前活跃的Track,一个是mTracks,表示这个线程创建的所有Track
(2)DuplicatingThread还维护一个mOutputTracks,表示多路输出的目的端.后面分析DuplicatingTread时在对比进行讲解
说明:大部分常见音频输出使用的是MixerThread

#====================================================================
4.3.5 PlaybackThread和AudioStreamOutput
可以发现一个PlaybackThread有一个AudioStreamOutput类型的对象,这个对象提供音频数据输出功能,以下示例以PlaybackThread最常用的子类MixerThread作为代表

AT--继承-->|
AT--继承-->| MixerThread(混音器)---->混音后的数据--->|AudioStreamOutput
AT--继承-->|

MixerThread的大致工作流程:
(1)接收来自AT的数据
(2)对这些数据进行混音
(3)把混音的结果写到AUdioStreamOut,这样就完成了音频数据的输出

#====================================================================
4.3.6 Track对象
前面所说的工作线程,其工作就是围绕Track展开的,
TrackBase --继承--> ExtendedAudioBufferProvider
Track --继承--> TrackBase --继承--> VolumeProvider
RecordTrack --继--> TrackBase 
TrackHandle和RecordHandle是基于Binder通信的,它作为Proxy,用于接收请求并派发给对应的Track和RecordTrack,之所以不让Track继承Binder框架,是因为Track本身的继承关系和所承担的工作已经很复杂了,如果再让它掺和Binder,只会乱上添乱
Track类作为工作线程的内部类来实现,其中:
(1)TrackBase定义于ThreadBase中
(2)Track定义于PlaybackTracks中,RecordTrack定义域RecordTracks中
(3)OutputTrack定义于DuplicatingThread中

#====================================================================
4.3.7 MixerThread分析
MixerThread是Audio系统中负担最重的一个工作线程
(1)MixerThread的来历
前面,在chekplaybackThread_l中,有一个地方一直没来得及解释,这个函数的内容就是根据outout值找到对应的回放线程,但在前面的分析中,并没有见到创建线程的地方,那这个线程又是如何得来的?它又是何时怎么创建的?
答案在AudioPolicyService中
AudioServiceService构造分析
(1)创建Tone音播放线程
(2)创建命令处理线程
(3)start output activity command thread
(4)创建AudioPolicyClient泛型,硬件厂商重写内部接口
   (4.1)在AudioPolicyClient中按照资料会调用openOutput函数调用到AudioFlinger
(5)工厂函数创建和硬件厂商相关的AudioPolicyManage并传入AudioPolicyClient
(6)加载音效模块

#====================================================================
4.3.8 AudioFlinger::openOutput分析
(1)调用findSuitableHwDev_l返回AudioHwDevice
(2)调用openOutput_l()
   (2.1)判断moudle是否是0
   (2.2)moudle == 0,这个请求来自之前的policymanager需要重新加载硬件
   (2.3)moudle != 0,根据moudle从mAudioHwDevs找到既有音频输出设备
   (2.4)返回AudioHwDevice对象
   (2.5)调用AudioHwDevice->openOutputStream(AudioStreamOut* ,...)传入AudioStreamOut对象,当做出参
   (2.6)如果将AudioStreamOut和AudioHwDevice关联成功,则根据flag数值创建关联线程
   (2.7)返回创建的线程对象
(3)调用openOutput_l()返回的thread对象的内部函数
(4)返回调用状态

总结:
(1)AF中的工作线程的创建,收到了AudioPolicyService的控制.从AudioPolicyService的角度触发,这也是应该的,因为APS控制着整个音频系统,而AF只是管理音频的输入和输出
(2)另外,注意这个线程是在AP的创建过程中产生的.也就是说,AP一旦创建完Audio系统,就已经准备好工作了.

#====================================================================
4.3.9 MixerThread的构造和线程启动
(1)创建AudioMixer对象 //混音器对象,这个对象比较复杂,它完成多路音频数据的混合工作
(2)基类PlaybackThread分析
   (2.1)参数校验
   (2.2)调用readOutputParameters_l,获取硬件参数信息,包括硬件中音频缓冲区的大小（以帧为单位）
   (2.3)设置不同类型音频流的音量及静音情况
(3)判断是否创建FastMixer

此时,线程对象已经创建.根据对Thread的分析,应该调用它的run函数才能真正创建线程,在首次创建MixerThread的sp对象时调用了run,这里利用了RefBase的onFirstRef函数.根据MixerThread的派生关系,该函数最终由父类PlaybackThread的onFirstRef实现

总结:createTrack到这里就结束了,下面开始start的分析

#====================================================================
4.3.10 start的分析
AT调用的是IAudioTrack的start函数,由于TrackHandle的代理作用,这个函数的实际处理会由Track对象来完成
status_t AudioFlinger::PlaybackThread::Track::start()分析
(1)判断线程是否存在
(2)线程存在,则判断是否卸载
(3)调用PlaybackThread.addTrack_l()
   (3.1)调用AudioSystem::startOutput()返回track状态
        (3.1.1)调用AudioPolicyService->startOutput()
        (3.1.2)调用AudioPolucyManager->startOutput()//后没看不懂就不继续跟踪了,最终如果调用成功会返回当前track的状态为TrackBase::ACTIVE
(4)参数校验,调用mAudioTrackServerProxy.start()和.obtainBuffer()
(5)设置当前tack的状态,以及缓冲区状态
(6)调用onAddNewTrack_l()
   (6.1)调用boradcast_l()广播一个事件,一定会触发MixerThread线程,通知它有活跃数组加入,需要开工干活

总结:start函数把这个Track加入到活跃数组后,将触发一个同步事件,这个事件会让工作线程动起来.
(1)mRetryCount表示重试次数,它针对的是这样一个问题.如果一个Track调用了start却没有write数据,该怎么办?如果MixerThread尝试了mRetryCount次后还没有可读数据,工作线程就会把该Track从激活队列中去掉了
(2)mFillingUpStatus能解决这样的问题:假设分配了1MB的数据缓冲,那么至少需要写多少数据工作线程才会让Track觉得AT是帧的需要它工作呢?这个状态最初为Track::FS_FILLING,表示正在填充数据缓冲区.在这种状态下,除非AT设置了强制读取数据标志(CB对象中的forceReady变量)否则工作线程是不会读取该Track的数据的

#====================================================================5. MixerThread线程
MixerThread的线程函数大致工作流程是:
(1)如果有通知信息或配置请求,则先完成这些工作.比如向监听者通知AF的一些信息,或者根据配置请求进行音量控制,声音设备切换等.
(2)调用prepareTracks_l函数,检查活跃Tracks是否有数据准备好
(3)调用混音器对象mAudioMixer的process,并传入一个存储结果数据的缓冲区,混音后的结果就存储在这个缓冲区中.
(4)调用代表音频输出设备的AudioOutoutStream对象的write,把结果数据写入设备

#====================================================================
6. AudioMixer对象的分析
AudioMixer为hook准备了多个实现函数，来看
·  process__validate：根据Track的格式、数量等信息选择其他的处理函数。

·  process__nop：什么都不做。

·  process__genericNoResampling：普通无需重采样。

·  process__genericResampling：普通需重采样。

·  process__OneTrack16BitsStereoNoResampling：一路音频流，双声道，PCM16格式，无需重采样。

·  process__TwoTracks16BitsStereoNoResampling：两路音频流，双声道，PCM16格式，无需重采样。

(1)getNextBuffer通过frameReady得到可读帧数
(2)getBuffer函数将根据可读帧数等信息得到可读空间的首地址
(3)releaseBuffer通过stepServer更新读位置

#====================================================================
7. audio_track_calk_t的分析
7.1 AT端的流程
AT端作为数据的生产者,可称它为写着,它在CB对象中用user表示.它的调用流程是:
(1)调用framesAvailable,看看是否有空余的可写空间.
(2)调用buffer,获得写空间起始地址
(3)调用setpUser,更新user的位置

7.2 AF端的流程
AF端作为数据的消费者,它在CB中的表示是server,可称它为读者.读者的使用流程是:
(1)调用framesReady看是否有可读数据
(2)获得可读数据的起始位置,这个和上面的buffer调用基本一样,都是根据offset和serverBase来获得可读数据块的首地址
(3)调用setpServer更新读位置

#====================================================================
8.AudioPolicyService的破解
8.1 AudioPoclicyService的创建,分析AudioPoclicyService的构造函数
AudioPolicyService和AudioFlinger都驻留于一个进程
(1)AudioPolicyInterface是Audio系统中的另一种HAL对象
(2)创建TonePlaybackThread用于播放Tone音，Tone包括按键音等
(3)AudioCommandThread用于处理控制命令,例如路由切换,音量调节等
(4)OutputCommandThread处理停止和释放输出线程
(5)判断是否使用旧版本的音频策略:#ifdef USE_LEGACY_AUDIO_POLICY
(6)创建AudioPolicyService的client端代理,并创建AudioPolicyManager(使用硬件厂商实现的AudioPolicyInterface),注:项目中使用的就是default的AudioPolicyManager
为什么在APS中也会存在HAL对象呢?APS主要是用来控制Audio系统的,由于各个硬件厂商的控制策略不可能完全一致,所以Android把这些内容抽象成一个HAL对象.

#====================================================================
8.2 对AudioPolicyInterface的分析,只简单看几个重点函数
/**
 * //设置设备的连接状态,这些设备有耳机,蓝牙等
 */
(1)setDeviceConnectionState() 
/**
 * //设置系统Phone状态,这些状态包括通话状态,来电状态等
 */
(2)setPhoneState() 
/**
 * //设置force_use的config策略,例如通话中强制使用扬声器
 */
(3)setForceUse() 
/**
 * audio_io_handle_t是int类型,这个函数的目的是根据传入的参数类型找到合适的输出句柄.这个句柄,在目前的Audio系统代表AF中的某个工作线程.还记得穿件AudioTrack的时候传入的那个output值吗?它就是通过这个函数得来的
 */
(4)virtual audio_io_handle_t getOutput()
/**
 * 它们的第二个参数表示使用的音频流类型
 */
(5)virtual status_t startOutput()
(6)virtual status_t stopOutput()
/**
 * 音量控制：设置不同音频流的音量级别范围，例如MUSIC有15个级别的音量
 */
(7)initStreamVolume()
/**
 * 设置某个音频流类型的音量级,例如觉得music声音太小时,可以调用这个函数提高音量级
 */
(8)setStreamVolumeIndex


#====================================================================
8.3 AudioSystem的介绍
AudioSystem是一个Native类,这个类在Java层有对应的Java类,其中定义了一些重要的类型,比如音频流流程,音频设备等,这些都在AudioSystem.中

8.3.1 stream type(音频流类型)
音频流类型有什么用呢?为什么要做这种区分呢?它主要与两项内容有关
(1)设备选择:例如,之前在创建AudioTrack时,传入的音频流类型是MUSIC,当插上耳机时,这种类型的声音只会从耳机中出来,但如果音频流类型是RING,则会从耳机和扬声器中同时出来.
(2)音量控制:不同流类型音量级的个数不同,例如,Music类型有15个级别可供用户调节,而有些类型只有7个级别的音量

8.3.2 audio mode(声音模式)
audio mode和电话的状态有直接关系
(1)系统有一个DSP,声音的输入输出都要经过它(不考虑蓝牙的情况).但它处理完的数字信号,需通过D/A(数/模)转换后输出到最终的设备上,这些设备包括扬声器,听筒,耳机等
注意:所谓的设备切换,是指诸如扬声器切换到听筒的情况,而前面提到的音频输出设备,应该指的是DSP.
(2)系统有两个核心处理器,一个是应用处理的核心,叫AP(Application Processor),可把它当做台式机上的CPU,在着上面可以运行操作系统.另一个和手机通信相关,一般叫BP(Baseband Porcessor基带处理器),可把它当做台式机上的"猫"
(3)AP和BP都能向音频DSP发送数据,它们在硬件上通路上互不干扰.于是就出现一个问题,即如果两个P同时往DSP发送数据,而互相之间没有协调,就可能出现通话声和音乐声混杂的情况.
(5)蓝牙没有像AP那样直接和音频DSP的相连,所以音频数据需要单独发给蓝牙设备.如果某种声音要同时从蓝牙和扬声器发出,亦即一份数据要往两个地方发送,便满足了AudioFlinger中DuplicatingThread出现的现实要求
注意:蓝牙设备实际上会建立两条数据通路:SCO和A2DP.A2DP和高质量立体声有关,切必须由AudioFlinger向它发送数据.所以"音频数据需要单独发送给蓝牙设备",这个设备实际上是指蓝牙的A2DP设备.

8.3.3 force use 和config(强制使用及配置)
手机通话时可以选择扬声器输出,这就是强制使用的案例.Audio系统对此有很好的支持.它涉及到两个方面:
(1)强制使用何种设备,例如使用扬声器,听筒,耳机等.它由forced_config控制
(2)在什么情况下需要强制使用,是通话的强制使用,还是听音乐的强制使用?这需由force_use控制.所以,AudioPolicyInterface的setForceUse函数,就是设置在什么情况下强制使用什么设备
virtual void setForceUse(AudioSystem::force_useusage,//什么情况

                          AudioSystem::forced_configconfig //什么设备

                         )= 0;

8.3.4 输出设备的定义
Audio定义了很多输出设备,来看其中几个:
enum audio_devices {
    //output devices
    DEVICE_OUT_EARPIECE = 0x1,  //听筒
    DEVICE_OUT_SPEAKER = 0x2,   //扬声器
    DEVICE_OUT_WIRED_HEADSET = 0x4,  //耳机
    DEVICE_OUT_WIRED_HEADPHONE = 0x8, //另外一种耳机
    DEVICE_OUT_BLUETOOTH_SCO = 0x10, //蓝牙相关，SCO用于通话的语音传输
    DEVICE_OUT_BLUETOOTH_SCO_HEADSET = 0x20,
    DEVICE_OUT_BLUETOOTH_SCO_CARKIT= 0x40,
    DEVICE_OUT_BLUETOOTH_A2DP = 0x80, //蓝牙相关，A2DP用于立体声传输
    DEVICE_OUT_BLUETOOTH_A2DP_HEADPHONES = 0x100,
    DEVICE_OUT_BLUETOOTH_A2DP_SPEAKER = 0x200,
    DEVICE_OUT_AUX_DIGITAL = 0x400,
    DEVICE_OUT_DEFAULT= 0x8000,
   ......
}

#====================================================================
8.4 AudioPolicyManagerBase的分析
先来看它的构造函数:
(1)将APS创建的AudioPolicyClientInterface对象传入
(2)清空强制使用配置
(3)加载audioPolicyconfig文件
(4)初始化声音曲线
(5)打开已连接设备的所有输出流
    (5.1)创建一个AudioOutputDescriptor对象,这个对象用来记录并维护与输出设备(相当于硬件的音频DSP)相关的信息,例如使用该设备的流个数,各个流的音量,该设备所支持的采样率,采样精度等.其中,有一个成员mDevice用来表示目前使用的输出设备,例如耳机,听筒,扬声器等
    (5.2)还记得MixerThread的来历么?openOutput导致AF创建了一个工作线程.该函数返回的是一个工作线程索引号.AMB维护了一个与设备相关的key/value集合.
    (5.3)设置输出设备,就是设置DSP的数据流到底从什么设备出去,这里设置的是从扬声器出去
    (5.4)更新不同策略使用的设备

8.4.1 AudioOutputDescriptor和openOutput
AudioOutputDescriptor对象,是AMB用来控制和管理音频输出设备的,从硬件上看,它代表的是DSP设备.
另一个重要点是openOutput函数.该函数的实现由APS来完成.之前曾分析过,它最终会在AF中创建一个混音线程(不考虑DirectOutput的情况),该函数返回的是该线程在AF中的索引号,亦即audio_io_handle_t表示的是AF中一个混音线程的索引号.这里涉及到一个非常重要的设计问题:AudioFlinger到底会创建多少个MixerThread?有两种设计方案:
(1)一种是一个MixerThread对应一个Track.如果这样,AMB仅使用一个audio_io_handle_t恐怕还不够用
(2)另一种是用一个MixerThread支持32路Track数据,多路数据通过AudioMixer混音对象在软件层面进行混音.
这里用的是第二种,当初设计为何不用一个MixerThead支持一路Track,然后把混音的工作交给硬件来完成呢?原因之一是如果采用一个线程一个Track的方式.就非常难于管理和控制,另一个原因时多线程个比较浪费资源

(3)A2dpOutput,它对应的MixerThread,专往代表蓝牙A2DP设备的AudioStreamOut上发送数据

8.4.2 setOutputDevice
现在要分析的调用是setOutputDevice,目的是为DSP选择一个合适的输出设备.

8.4.3 Audio Strategy
现在调用的函数是updateDevciceForStrategy,这里会引出一个strategy的概念,现在看rounting_stratgy的定义

//routing_strategy:路由策略
enum routing_strategy {
    STRATEGY_MEDIA, 
    STRATEGY_PHONE,
    STRATEGY_SONIFICATION,
    STRATEGY_DTMF,
    NUM_STRATEGIES
 }
AudioSystem使用的流类型并不是和路由直接相关的,AudioPolicy内部,是使用routing_strategy来控制路由策略的
AudioSystem的getOutput就是想找到AF中的一个工作线程.为什么这个线程号会有AP返回呢?是因为Audio系统需要:
(1)根据流类型找到对应的路由策略.
(2)根据该策略找到合适的输出device(指扬声器,听筒之类的)
(3)根据device选择AF中合适的工作线程,例如是蓝牙的MixerThread,还是DSP的MixerThread或者是DuplicatingThread.
(4)AT根据得到的工作线程索引号,最终将在对应的工作线程中创建一个Track.之后AT的数据将由该线程负责处理

总结:流程分析
(1)AT的目的是把数据发送给对应的设备.例如是蓝牙,DSP等
(2)代表输出设备的HAL对象由MixerThread线程持有,所以要找到对应的MixerThread
(3)AP维护流类型和输出设备(耳机,蓝牙耳机,听筒等)之间的关系,不同的输出设备使用不同的混音线程.
(4)AT根据自己的流类型,向AudioSystem查询,希望得到对应的混音线程号

8.5 重回start
现在要分析的就是start函数.AT的start虽然没有直接与AP交互,但在AF的start中却和AP有着交互关系
PlaybackThread::Track::start()-->
PlaybackThread::addTrack_l()-->
AudioSystem::startOutput()-->
AudioPolicyManager::startOutput()
(1)根据output找到对应的SwAudioOutputDescriptor
(2)getNewOutputDevice将得到一个设备,startSource将使用这个设备进行路由切换
(3)getNewOutputDevice分析
    (3.1)isUsedByStrategy判断某个策略是否正在被使用
    (3.2)一旦得到当前outputDesc使用的策略,便可根据该策略找到对应的设备.注意if和else的顺序,它代表了系统优先使用的策略,以第一个判断为例,假设系统已经插上了耳机,并且处于通话状态时,而且强制使用了扬声器,那么声音都从扬声器出.这时,如果想听音乐的话,则应首先使用STRATEGY_PHONE的对应设备,此时就是扬声器.所以音乐将从扬声器出来,而不是耳机.上面的仅是举例.具体的情况还要综合考虑Audio系统中的其他信息.另外如果fromCache为true,将直接从内部保存的旧信息中的到设备

8.6 本节小结
(1)AT通过AP获取AF中的工作线程索引号,这决定了数据传输的最终目标是谁,比如是音频DSP或是蓝牙
(2)AT的start和stop会影响Audio系统的路由切换,是根据创建Track时传入的StreamType决定的路由策略

#====================================================================
9. 相关概念补充描述
参考资料:https://www.cnblogs.com/wulizhi/p/8183658.html

9.1 关于缓冲区的类概述
声明类:AudioTrackShared.h 实现类:AudioTrackShared.cpp
(1)AudioTrackClientProxy:MODE_STREAM模式下,生产者AudioTrack使用它在FIFIO中找到可用看见的位置
(2)AudioTrackServerProxy:MODE_STATIC模式下,生产者AudioTrack使用它在FIFO中找到可用空间的位置
(3)StaticAudioTrackClientPorxy:MODE_STATIC模式下,生产者AudioTrack使用它在FIFO中找到可用空间的位置
(4)StaticAudioTrackServerPorxy:MODE_STATIC模式下,消费者AudioFlinger::PlaybackThread使用它在FIFO中找到可读数据的位置
(5)AudioTrackServerProxy:生产者AudioFlinger::RecordThread使用它在FIFO中找到可用空间的位置

9.2 AudioFlinger 回放录制线程
(1)ThreadBase:PlaybackThread和RecordThread的基类
(2)RecordThread:录制线程类,由ThreadBase派生
(3)PlaybackThread:回访线程基类,由ThreadBase派生
(4)MixerThread:混音回访线程类,由PlaybackThread派生,负责处理标识为AUDIO_OUTPUT_FLAG_PRIMARY,AUDIO_OUTPUT_FLAG_FAST,AUDIO_OUTPUT_FLAG_DEEP_BUFFER的音频流,MixerThread可以把多个音轨的数据混音后在输出
(5)DirectOutputThread:直输回放线程类,由PlaybackThread派生,负责处理标识为AUDIO_OUTPUT_FLAG_DIRECT的音频流,这种音频流数据不需要软件混音,直接输出到音频设备即可
(6)DuplicatingThread:复制回放线程类,由MixerThread派生,负责复制音频流数据到其他输出设备,使用场景如主声卡设备,蓝牙耳机设备,usb声卡设备同时输出
(7)OffloadThread:硬解回放线程类,由DirectOutputThread派生,负责处理标识为AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD的音频流,这种音频流未经软件解码的(一般是MP3,AAC等格式的数据),需要输出到硬件解码器,由硬件解码器解码成PCM数据

9.3 从Audio HAL中,我们通常看到如下4种输出流设备,分别对应着不同的播放场景:
(1)primary_out:主输出流设备,用于铃声类型声音输出,对应标识为AUDIO_OUTPUT_FLAG_PRIMARY的音频流和一个MixerThread回放线程实例
(2)low_latency:低延迟输出流设备,用于按键音,游戏背景音等对时延要求高的声音输出,对应着标识为AUDIO_OUTPUT_FLAG_FAST的音频流和一个MixerThread回放线程实例
(3)deep_buffer:音乐音轨输出流设备,用于音乐等对时延要求不高的声音输出,对应着标识为AUDIO_OUTPUT_FLAG_DEEP_BUFFER的音频流和一个MixerThread回放线程实例
(4)compress_offload:硬解输出流设备,用于需要硬件解码的数据输出,对应着标识为AUDIO_OUTPUT_FLAG_COMPRESS_OFFLOAD的音频流和一个OffloadThread回放线程实例

注,理解:根据audio_policy.conf配置文件.会根据配置中的字段,创建每个设备对应的Thread,然后这个设备可以匹配(如AUDIO_DEVICE_OUT_SPEAKER)不同标志的AudioTrack类型.
不同的设备会创建不同的线程,每个线程有会支持不同的标志(如MixerThread支持AUDIO_OUTPUT_FLAG_PRIMARY类型)

9.4 audio_io_handler_t概述
这里在详细说明一下audio_io_handle_t,它是AudioTrack/AudioRecordThread/AudioSystem,AudioFlinger,AudioPolicyManager之间一个重要的链接点.当打开输出流设备及创建PlaybackThread时,系统会分配一个全局唯一的值作为audio_io_handle_t,并把audio_io_handle_t和PlaybackThread添加到键值对向量mPlaybackThreads中,由于audio_io_handle_t和PlaybackThread是一一对应的关系,因此拿到一个audio_io_handle_t,就能遍历键值对向量mPlaybackThreads找到它对应的PlayebackThread,可以简单理解audio_io_handle_t为PlaybackThread的线程索引号或线程id

9.5 AudioPolicyService中定义的设备类型
AUDIO_DEVICE_OUT_BLUETOOTH_A2DP
AUDIO_DEVICE_OUT_BLUETOOTH_A2DP_HEADPHONES(普通蓝牙耳机)
AUDIO_DEVICE_OUT_BLUETOOTH_A2DP_SPEAKER(蓝牙小音箱)
//此处属于setForceUse的强制插队
(if FORCE_SPEAKER)AUDIO_DEVICE_OUT_SPEAKER(扬声器)
AUDIO_DEVICE_OUT_WIRED_HEADPHONE(普通耳机，只能听，不能操控播放)
AUDIO_DEVICE_OUT_LINE
AUDIO_DEVICE_OUT_WIRED_HEADSET(线控耳机)
AUDIO_DEVICE_OUT_USB_HEADSET(USB耳机)
...
AUDIO_DEVICE_OUT_SPEAKER(扬声器)

9.6 音频数据流向:
1.客户端将数据写入AudioFlinger创建的共享内存
类名:AudioTrack.cpp
函数:ssize_t AudioTrack::write(const void* buffer, size_t userSize, bool blocking)

2.MixerThread构造时创建,申请内存空间赋值给mSinkBuffer
类名:Threads.cpp
函数:AudioFlinger::MixerThread::MixerThread(const sp<AudioFlinger>& audioFlinger, AudioStreamOut* output,
        audio_io_handle_t id, audio_devices_t device, bool systemReady, type_t type)
函数内部调用:(void)posix_memalign(&mSinkBuffer, 32, sinkBufferSize);

3.创建Track时会将MierThread中申请的内存地址mSinkBuffer赋值给成员变量mMainBuffer
类名:Tracks.cpp
函数:AudioFlinger::PlaybackThread::Track::Track(
            PlaybackThread *thread,const sp<Client>& client,
            audio_stream_type_t streamType,uint32_t sampleRate,
            audio_format_t format,audio_channel_mask_t channelMask,
            size_t frameCount,void *buffer,
            const sp<IMemory>& sharedBuffer,audio_session_t sessionId,
            int uid,audio_output_flags_t flags,track_type type)
函数内部调用:mMainBuffer(thread->mixBuffer()),

4.在prepareTracks_l()的时候将active的track的mMainBuffer赋值到AudioMixer中去,这样混音数据就保存到这个buffer了
类名:Threads.cpp
函数:AudioFlinger::PlaybackThread::mixer_state AudioFlinger::MixerThread::prepareTracks_l(
        Vector< sp<Track> > *tracksToRemove)
函数内部调用:mAudioMixer->setParameter(
             name,
             AudioMixer::TRACK,
             AudioMixer::MAIN_BUFFER, (void *)track->mainBuffer());

5.AudioMix中调用调用getNextBuffer()可以从共享内存中获取声音数据,在进行混音处理
类名:Tracks.cpp
函数:status_t AudioFlinger::PlaybackThread::Track::getNextBuffer(
        AudioBufferProvider::Buffer* buffer)
函数内部调用:status_t status = mServerProxy->obtainBuffer(&buf);

6.AudioFlinger工作线程将混音后的数据,传入到MonoPipe.进行保存以供FastMixer进行读取
类名:Threads.cpp
函数:bool AudioFlinger::PlaybackThread::threadLoop()
-->函数:ssize_t AudioFlinger::MixerThread::threadLoop_write()
-->函数:ssize_t AudioFlinger::PlaybackThread::threadLoop_write()
函数内部调用:ssize_t framesWritten = mNormalSink->write((char *)mSinkBuffer + offset, count);
注:在MoniPipe.cpp中ssize_t MonoPipe::write(const void *buffer, size_t count)函数会根据当前写入本地缓存数据量的大小,睡眠不同时间的长短

7.FastMixer工作线程将AudioFlinger混音后的数据,再次进行混音,并写到tinyalsa.
类名:FastMixer.cpp
函数:void FastMixer::onWork()
-->函数内部调用:mMixer->process()
-->函数内部调用:ssize_t framesWritten = mOutputSink->write(buffer, frameCount)
注:在onWork函数中获取MonoPipe.cpp中的音频数据,进行再次混音.AudioFlinger线程768的数据分4次写入到下层,一次只处理192个字节其他位补0,写了四次(需要在次验证,还不完全确定)
注2:在FastThread工作函数中,会根据硬件消耗数据的快慢进行适当的sleep

8.将FastMixer处理后的audio数据继续传到硬件设备
类名:AudioStreamOutSink.cpp
函数:ssize_t AudioStreamOutSink::write(const void *buffer, size_t count)
函数内部调用:ssize_t ret = mStream->write(mStream, buffer, count * mFrameSize);

资料整理:
Android O(8.0)音频write数据流程变化（HIDL
https://blog.csdn.net/bberdong/article/details/78346729
android 音频数据在AudioFlinger中的处理（出入口）
https://blog.csdn.net/shisi/article/details/40080139
https://blog.csdn.net/vviccc/article/details/105341277

audiomixer分析
https://blog.csdn.net/WAN8180192/article/details/50705897?locationNum=5&fps=1
https://www.cnblogs.com/TaigaCon/p/4844919.html
https://blog.csdn.net/vviccc/article/details/105341277
https://msd.misuland.com/pd/3181438578597037046

9.7 FadeIn和FadeOut
donghaocheng: 
FadeOut按照原生逻辑,pasue之后在继续写一包数据下去,做FadeOut也就是用这一包
FadeIn在Android8之后AudioTrack中添加了Volumeshaper功能就是用来做fade的.AudioTrack接口createVolumeShaper,c++接口applyVolumeShaper



#====================================================================
10 什么是AudioEffect?
https://blog.csdn.net/qidi_huang/article/details/53741788?utm_medium=distribute.pc_relevant.none-task-blog-baidulandingword-1&spm=1001.2101.3001.4242
它实际上是定义在音频Framework层的一种类(Class).由它派生出了Equalizer,Virtualizer,BassBoost,PresetReverb,AudioEffect对象了,再将其应用到AudioTrack或MediaPlayer实例上进行音效控制.由此课件,AudioEffect实际上是一个上层纯软件的概念,通过纯软件的方法对音效进行控制

(1)Equalizer AudioEffect(均衡器)
均衡器可以被用来改变特定音源输入的频率响应或经混音后的主要输出音频的频率响应.APP在Framework层创建Equalizer对象并实例化一个Equalizer引擎,通过它们可以对每一个频带的增益进行精确地控制
(2)BassBoost AudioEffect(低音增强器)
低音增强器被用于增强音频中的低频部分,相当于一个只能对低频声音进行增益控制的简单均衡器,APP在Framework层创建BassBoost对象并实例化一个低音增强引擎
(3)PresetReverb AudioEffect(预设混响器)
一个声音在产生后会往各个方向传播.如果这个声音是在室内产生的,接受方会首先听到音源本身发出的声音,然后听到来自墙壁,天花板和地板的回声.这些回声又会在传播到障碍物上时产生二次回声,三次回声..接收方会持续不断地听到声音,这每一次声音的辨识度越来越低,且响度随时间衰减.混响器(Reverb)对于音频接收者的环境建模非常重要,它可以被用于模拟在不同环境中播放音乐产生的效果,或者在游戏中为玩家生成更加沉浸式的音频体验.APP可以通过预设混响器提前配置全局的混响参数,这种方式在音乐播放添加混响效果时被广泛使用.如果APP需要配置更高级的换进声音混响参数,最好使用环境声混响器来实现.APP 在 Framework 层创建 PresetReverb 对象并实例化一个混响引擎。AudioPolicyConfig

#====================================================================
11. Android开发之音频配置文件audio_policy.conf解析全过程(基于android7.0分析)
https://blog.csdn.net/qq_33750826/article/details/82455026

11.1 概念:audio_policy.conf顾名思义audio hw模块配置文件,用于加载音频硬件抽象层动态库.得到系统所支持的输入,输出音频设备.

11.2 系统解析
audio_policy.conf文件是用于加载音频硬件抽象层动态库,而系统音频是否能够输入输出则取决于这些动态库是否能够加载成功.而是否能够加载自定义的音频库,则取决你是否在配置文件中计入自定义的音频库配置信息
AudioPolicyConfig对象为封装的audio_policy.conf的对象.AudioPolicyConfig对象包含了audio_policy.conf中所有hw音频模块的集合,可获取的输入,输出设备的集合,默认输出设备的集合

11.3 整个audio_policy.conf总结
...
global_configuration {  
  attached_output_devices AUDIO_DEVICE_OUT_SPEAKER
   #--->addAvailableOutputDevices --->AudioPolicyConfig
  default_output_device AUDIO_DEVICE_OUT_SPEAKER
   # --->setDefaultOutputDevice  --->AudioPolicyConfig
  attached_input_devices AUDIO_DEVICE_IN_BUILTIN_MIC|AUDIO_DEVICE_IN_REMOTE_SUBMIX|AUDIO_DEVICE_IN_DIA_REMOTE
   # --->addAvailableInputDevices --->AudioPolicyConfig
}
...
audio_hw_modules {  #  --->HwModuleCollection  --->AudioPolicyConfig.setHwModules(hwModules);
...
  primary {#--->HwModule.mName--->HwModule（primary）
  ...
    global_configuration {   
      attached_output_devices AUDIO_DEVICE_OUT_SPEAKER  
      #--->AudioPolicyConfig.addAvailableOutputDevices 
      default_output_device AUDIO_DEVICE_OUT_SPEAKER
     # --->AudioPolicyConfig.setDefaultOutputDevice
      attached_input_devices AUDIO_DEVICE_IN_BUILTIN_MIC
     # --->AudioPolicyConfig.addAvailableInputDevices
      audio_hal_version 3.0
      #--->setHalVersion --->HwModule.mHalVersion
    }
    devices {  
      speaker {  #--->DeviceDescriptor-->DeviceVector-->HwModule.mDeclaredDevices
        type AUDIO_DEVICE_OUT_SPEAKER  
        #-->DeviceDescriptor.mDeviceType
        gains { 
          gain_1 { 
#-->AudioGain——>AudioPort.mGains(AudioGainCollection)-->DeviceDescriptor
            mode AUDIO_GAIN_MODE_JOINT 
             #--->AudioGain.mode
            min_value_mB -8400
            #--->AudioGain.min_value
            max_value_mB 4000
            #--->AudioGain.max_value
            default_value_mB 0
            #--->AudioGain.default_value
            step_value_mB 100
            #--->AudioGain.step_value
          }
        }
      }
...
    }
    outputs { 
    #OutputProfile-->IOProfile.AudioPort.mRole=AUDIO_PORT_ROLE_SOURCE
    ...
      primary {  #OutputProfile-->IOProfile--->AudioPort.mName
        sampling_rates 48000
#SampleRateVector-->AudioProfile-->AudioProfileVector-->IOProfile-->AudioPort.mProfiles
        channel_masks AUDIO_CHANNEL_OUT_STEREO
#ChannelsVector-->AudioProfile-->AudioProfileVector-->IOProfile-->AudioPort.mProfiles	
        formats AUDIO_FORMAT_PCM_16_BIT
#FormatVector-->AudioProfile-->AudioProfileVector-->IOProfile-->AudioPort.mProfiles	
        devices speaker|HDMI|SPDIF|wired_headphone|wired_headset|BT_sco|BT_sco_headset
#IOProfile.mSupportedDevices
        flags AUDIO_OUTPUT_FLAG_PRIMARY
#IOProfile.mFlags
      }
...
    }
    inputs { #InputProfile-->IOProfile.AudioPort.mRole=AUDIO_PORT_ROLE_SINK
      primary { #InputProfile-->IOProfile--->AudioPort.mName
        sampling_rates 8000|11025|12000|16000|22050|24000|32000|44100|48000
#SampleRateVector-->AudioProfile-->AudioProfileVector-->IOProfile-->AudioPort.mProfiles
        channel_masks AUDIO_CHANNEL_IN_MONO|AUDIO_CHANNEL_IN_STEREO
#ChannelsVector-->AudioProfile-->AudioProfileVector-->IOProfile-->AudioPort.mProfiles	
        formats AUDIO_FORMAT_PCM_16_BIT
#FormatVector-->AudioProfile-->AudioProfileVector-->IOProfile-->AudioPort.mProfiles	   
        devices AUDIO_DEVICE_IN_BUILTIN_MIC|AUDIO_DEVICE_IN_BLUETOOTH_SCO_HEADSET|AUDIO_DEVICE_IN_WIRED_HEADSET
#IOProfile.mSupportedDevices
      }
    }
    ...
  }
  ...
}

/*描述多个modules*/
audio_hw_modules {
    primary { //一个modules对应一个厂家提供的so文件
      outputs { //一个modules可以有多个output
        primary {//一个output表明他的参数（primary代表默认设备）
          sampling_rates  44100|48000 //采样率
          channel_masks AUDIO_CHANNEL_OUT_STEREO //通道
          formats AUDIO_FORMAT_PCM_16_BIT //格式
          /*有哪些device，如喇叭，耳机等等*/
          devices  AUDIO_DEVICE_OUT_EARPIECE|AUDIO_DEVICE_OUT_SPEAKER|AUDIO_DEVICE_OUT_WIRED_HEADSET|AUDIO_DEVICE_OUT_WIRED_HEADPHONE|AUDIO_DEVICE_OUT_ALL_SCO|AUDIO_DEVICE_OUT_AUX_DIGITAL|AUDIO_DEVICE_OUT_SPDIF
          flags AUDIO_OUTPUT_FLAG_PRIMARY
        }

#====================================================================
12. Android 开发之音频配置文件audio_policy_configuration.xml解析全过程
https://blog.csdn.net/qq_33750826/article/details/97757590

12.1 前言
前面我们已经介绍了audio_policy.conf的解析全过程,但是.conf是一种简单的专有格式,有较大的局限性,无法描述电视和汽车等引用的复杂拓扑.Android7.0增加了对使用XML格式来定义音频拓扑的支持,这种文件格式更通俗易懂,具有多种编辑和解析工具,并且足够灵活,可以描述复杂的音频拓扑.
注意:Android7.0仍支持使用audio_policy.conf;系统会默认使用这种旧版格式.要使用XML文件格式,需要在设备Makefile中添加构建选项USE_XML_AUDIO_POLICY_CONF:=1

12.2 XML格式的优势
与在.conf文件中一样,新的XML文件支持定义输出输入流配置文件,可用于播放和捕获的设备以及音频属性的数量和类型.此外,XML格式还提供一下增强功能:
(1)音频配置文件目前的结构类似于HDMI简单音频描述符,支持每种音频格式使用一组不同的采样率/声道掩码
(2)设备和流之间所有可能连接的显式定义.以前,借助隐式规则,可以使连接到同一HAL模块的所有设备互连,从而阻止音频政策控制使用音频补丁程序API请求的连接.现在,在XML格式中,拓扑描述定义了连接限制
(3)对"包含"的支持可避免出现重复的标准A2DP,USB或重新导向提交定义.
(4)可自定义的音量曲线.以前,音量表采用硬编码格式.在XML格式中,音量表通过描述来定义,并且可自定义

12.3 文件格式和位置
新的音频政策配置文件是audio_policy_configuration.xml,位于/system/etc
顶层结构中包含与各个音频HAL硬件模块对应的模块,其中每个模块都有一系列混合端口,设备端口和导向:
(1)混合端口(<mixPorts>)描述了可以在音频HAL处打开以供播放和捕获的流的可能配置文件
(2)设备端口(<devicePort>)描述了可以附上其类型(以及(可选)地址和音频属性,如果相关)的设备
(3)导向(<routes>)现在已从混合端口描述符中分离出来,支持描述从设备到设备或流到设备的导向

12.4 解析
之前解析audio_policy.conf中我们已经知道通过AudioPolicyConfig类来装载整个音频策略文件数据,这里也是一样,通过AudioPolicyConfig来封装audio_policy_configuration.xml所包含的策略数据
(1)HwModuleCollection mHwModuls;   .xml中所有module模块的集合
(2)DeviceVector mAvailiableOutputDevices;   .xml中所有output devices模块的集合
(3)DeviceVector mAvailableInputDevices;   .xml中所有input devices模块的集合
(4)sp<DeviceDescription> mDefaultOutputDevices;   .xml中默认output devicePort
(5)VolumeCurvesCollection mVolumeCurves;   .xml中音量集合

12.5 总结

//AudioPolicyConfig
<audioPolicyConfiguration version="1.0" xmlns:xi="http://www.w3.org/2001/XInclude">

......

//HwModuleCollection--->AudioPolicyConfig.setHwModules(HwModuleCollection);
<modules>
	
		......
	
		<!-- Primary Audio HAL -->
		/**
			name="primary" 
			halVersion="2.0"
		*/
		// HwModule(name,version )   -->HwModuleCollection.add(HwModule)
        <module name="primary" halVersion="2.0">

			......
        
            <attachedDevices>
            // sp<DeviceDescriptor> device=HwModule->getDeclaredDevices().getDeviceFromTagName(Earpiece)
            //AudioPolicyConfig->addAvailableDevice(device);
                <item>Earpiece</item>
                ....
            </attachedDevices>
            
            //sp<DeviceDescriptor> device=HwModule->getDeclaredDevices().getDeviceFromTagName(Speaker)
            //AudioPolicyConfig->setDefaultOutputDevice(device);
            <defaultOutputDevice>Speaker</defaultOutputDevice>
            
            //IOProfileCollection-->HwModule->setProfiles(IOProfileCollection);
            <mixPorts>
            /**
				name="primary output"
				 role="source"
				     audio_port_role_t portRole = role == "source" ? AUDIO_PORT_ROLE_SOURCE : AUDIO_PORT_ROLE_SINK;

			*/
            //IOProfile(name,portRole) extends AudioPort     -->IOProfileCollection.add(IOProfile ); 
                <mixPort name="primary output" role="source" flags="AUDIO_OUTPUT_FLAG_PRIMARY">
                 /**
				AudioProfile.setDynamicFormat(format)
				AudioProfile->setDynamicChannels(channelMasks);
				AudioProfile->setDynamicRate(samplingRates);
				*/
                //sp <AudioProfile>  -->AudioProfileVector.add(AudioProfile);  --> IOProfile ->setAudioProfiles(AudioProfileVector);
                    <profile name="" format="AUDIO_FORMAT_PCM_16_BIT"
                             samplingRates="48000" channelMasks="AUDIO_CHANNEL_OUT_STEREO"/>
                             
                           .....
                           
                </mixPort>
                
                ......
                
            </mixPorts>
            
            //DeviceVector   --> HwModule->setDeclaredDevices(DeviceVector)
            <devicePorts>
                <!-- Output devices declaration, i.e. Sink DEVICE PORT -->
                //typeName="AUDIO_DEVICE_OUT_EARPIECE"  role="sink"
                /**
				audio_port_role_t portRole = (role == Attributes::roleSource) ?
                AUDIO_PORT_ROLE_SOURCE : AUDIO_PORT_ROLE_SINK;
    			audio_devices_t type = AUDIO_DEVICE_NONE;
   			 	if (!DeviceConverter::fromString(typeName, type) ||
           	 		(!audio_is_input_device(type) && portRole == AUDIO_PORT_ROLE_SOURCE) ||
            		(!audio_is_output_devices(type) && portRole == AUDIO_PORT_ROLE_SINK)) {
        				ALOGW("%s: bad type %08x", __FUNCTION__, type);
       					 return BAD_VALUE;
   				 }
				*/
                //DeviceDescriptor(type,name)  extends AudioPort,AudioPortConfig   --> DeviceVector.add(DeviceDescriptor)
                <devicePort tagName="Earpiece" type="AUDIO_DEVICE_OUT_EARPIECE" role="sink">
                /**
				AudioProfile.setDynamicFormat(format)
				AudioProfile->setDynamicChannels(channelMasks);
				AudioProfile->setDynamicRate(samplingRates);
				*/
                //sp <AudioProfile>  -->AudioProfileVector.add(AudioProfile);  --> DeviceDescriptor  ->setAudioProfiles(profiles);
                   <profile name="" format="AUDIO_FORMAT_PCM_16_BIT"
                            samplingRates="48000" channelMasks="AUDIO_CHANNEL_IN_MONO"/>
                            
                            ......
                </devicePort>

				......

           </devicePorts>
           
            <!-- route declaration, i.e. list all available sources for a given sink -->
            //AudioRouteVector   --> HwModule->setRoutes(AudioRouteVector)
            <routes>
            //type="mix"
            //AudioRoute(type)  -->  AudioRouteVector.add(AudioRoute);
            //sink="Earpiece"  --->  sp<AudioPort> sink = AudioPolicyConfig->findPortByTagName(sink);
            //sink->addRoute(AudioRoute);  -->   AudioRoute->setSink(AudioPort);
            //sources=""primary output,......"    ->  AudioPortVector.add(sp<AudioPort> source = AudioPolicyConfig->findPortByTagName("primary output");)  -->   AudioRoute  ->setSources(AudioPortVector);
                <route type="mix" sink="Earpiece"
                       sources="primary output,low_latency,compressed_offload,voip_rx,BT SCO Headset Mic"/>
                       
                       ......
                       
            </routes>

			......

        </module>
        
        ......
        
</modules>

.....

</audioPolicyConfiguration>


12.6 补充: audio_policy_configuration.xml解析
https://blog.csdn.net/u012188065/article/details/84104275
1.xml的层级如下
(1)xml中包含<modules>包含了多个<module>成员,每个<module>都是一个audio hal(对应代码中的HwModule类)
(2)每个<module>中包含了多个层级

解析module时,一次解析Audio Mix Port(对应<mixPort>),Audio Device Ports(Source/Sink)(对应<devucePorts>),Audio Routes(对应<routes>)

2.解析<mixPorts>模块
<mixPorts>中包含了大量的<mixPort>(对应代码中AudioPort的子类IOProfile),共有两大类role,一类是"source",一类是"sink"
"source"类<mixPort>共同组成了HwModule中的mOutputProfiles,"sink"类的<mixPort>共同组成了HwModule中的mInputProfiles,mOutProfiles+mInputProfiles共同组成了HwMoudle中的mPorts

3.解析<devicePorts>模块
<devicePorts>包含了许多<devicePort>(对应代码中AudioPort和AudioPortConfig的子类DeviceDescriptor),共同组成了HwModule中的mDeclaredDevices,里面的各个device成员也会加载到先前HwModule类中的mPort.

4.解析<routes>模块
<routes>中包含了许多<route>(对应代码中的AudioRoute类),顾名思义,是桐庐的意思,其连接port和device的作用.每个<route>实质上是一个MIX或者MUX,以MIX为例,一个MIX包含一个sink(即AudioRoute的mSink)和多个source(分别add到AudioRoute的mSource),此时已经将sink和source建立了简单的关系

5.在AudioPolicyManager构造中会遍历mInputProfiles中的sink和mOutputProfiles中的source和sink
5.1 以遍历mInputProfiles(sink)为例
(1)根据<mixPort>中的role="sink"的name,找到<routes>中name一致的<route>
(2)此<route>中对应了多种source,根据每个source的name,找到<devicePorts>中,tagName一致的<devicePort>
(3)分别将<devicePort>添加到此sink(类型为sp<IOProfile>)的mSupportedDevices表示此sink支持的sourceDevice设备(即此线程支持哪些设备)
(4)继续遍历下一个sink
最后setSupportedDevices设置此sink支持的devices

5.2 以遍历mOutputProfiles(source)为例
(1)根据<mixPort>中的role="source"的name,找到<routes>中的多个<route>
(2)找到所有<route>中source等于<mixPort>name的sink字段值
(3)添加到此source(类型为sp<IOProfile>)的mSupportedDevices,表示此source支持的sinkDevices设备
(4)继续遍历下一个source,最后setSupportedDevices设置此source支持的devices

6. 解析<attachedDevices>和<defaultOutputDevice>模块
<attachedDevices>中获取的设备与<devicePorts>中的设备(mDeclaredDevices)取交集,获得了当前设备默认支持的输入输出设备集,即mAvailableOutputDevices,mAvailableInputDevices,为之后播放各种音乐时,选择特定device做铺垫.之后在拔插usb耳机或者3.5mm耳机时,底层会有上报事件,及时去更新mAvaliableOutputDevices和mAvailableInputDevices

<defaultOutputDevice>设置为此HwModule中的默认的设备,一般为speaker设备

#====================================================================
13. Android AudioPolicyService流程详解

13.1 AudioPolicyClient,它是AudioPolicyService的子类,AudioPolicyClient对象在AudioPolicyService::onFirstRef()被创建,并以指针的形式作为参数去创建AudioPolicyManager.所以在AudioPolicyManager里面看到mpClientInterface的调用就是AudioPolicyClient类的方法.其中有两种方式:
(1)一种是直接通过AudioSystem::get_AudioSystem_flinger()来获得BpAudioFlinger,然后在调用AudioFlinger代码
(2)两一种方法就是调用AudioPolicyService的方法,将cmd作为一个node方到AudioPolicyService::AudioCommandThread的队列中,然后一个CommandThread会不断的处理这些cmd node,在处理的时候也是通过第一种凡是call到AudioFlinger
补充:实际AudioPolicyService和AudioFlinger属于同一个进程中,但还是使用Binder通信

13.2 AudioPolicyService启动流程分析
(1)创建tone音效播放线程
(2)创建用于执行audio的命令
(3)创建用于执行输出的命令
(4)判断是否使用老版本的音频策略创建(#ifdef USE_LEGACY_AUDIO_POLICY),创建音频策略管理者
(5)创建音效类AudioPolicyEffects

13.3 AudioPolicyManager启动流程分析
(1)赋值AudioPolicyClientInterface参数给本地环境变量,可以通过此对象调用到直接或转线程调用到AudioFlinger侧.
(2)根据宏定义判断解析xml或conf文件,来加载平台音频策略,如果xml或conf加载都失败,则设置为默认值
(3)设置音频流对应的音量调节
代码:mVolumeCurves->initializeVolumeCurves(speakerDrcEnabled);
(4)解析策略配置后,检索引擎的实例并对其进行初始化,就是创建Engine类作为实际的工作者AudioPolicyManager作为管理者
(5)打开访问连接设备的所有输出流(mAvailableOutputDevices和mAvailableInputDevice现在包含所有连接的设备)
    (5.1)遍历mHwModules->mOutputProfiles,通过名称打开对应的音频接口硬件抽象库base name of the audio hw module(primary, a2dp ...)
        (5.1.1)打开访问连接设备所需的所有输出流,但是只有在应用程序实际需要时才打开直接输出流
        (5.1.2)根据outProfile创建一个描述符,并传入mpClientInterface,SwAudioOutputDescriptor代表一个设备并且对应着一个输出线程
        (5.1.3)调用到AudioFlinger侧根据设备创建一个输出工作线程
        (5.1.4)代码:addOutput(output, outputDesc);
                   setOutputDevice(outputDesc,
                        outputDesc->mDevice,
                        true,
                        0,
                        NULL,
                        address.string());
    (5.2)遍历mHwModules->mInputProfiles.配置文件中的所有输入设备
        (5.2.1)根据inProfile创建AudioInputDescriptor描述符
        (5.2.2)调用AudioFlinger侧根据设备创建对应一个输入线程
(6)再次确已经为所有连接的设备分配了唯一ID
(7)确认默认设备是可以访问的
(8)更新音频设备对应的线程,调用:void AudioPolicyManager::updateDevicesAndOutputs(),遍历路由策略,然后将每种路由策略对应的设备对应到mDeviceForStrategy数组中,如:a音频策略对应着几种设备


#====================================================================
14.动态路由(即汽车音频路由的常规方式)
https://www.mscto.com/iot/604315.html
https://msd.misuland.com/pd/4146263742822225278
开始介绍之前,还需要重点介绍一下动态路由的初始化,它是汽车路由的核心,所以有必要在开始进行介绍

一. 动态路由的初始化
//packages/services/Car/service/src/com/android/car/CarAudioService.java
1. CaeAudioService的init开始
(1)mUseDynamicRouting变量一般定制会在device overlay目录进行覆盖定制,默认为false,不启用动态路由.我们要使用动态路由则设为true

2. setupDynamicRouting()
(1)获取路由策略
(2)向AudioPolicyManager注册路由策略

3. getDynamicAudioPolicy()
(1)获取所有的device端口
(2)进行context到bus的匹配
(3)建立路由,通过context建立usage到bus的路由建立
(4)通过context获取对应的usage集合
(5)构造一个AudioMix对象,这个对象包含了所有的usage对应关系和device所需的信息
(5)设置AudioPolicyVolumeCallback

通过CarAudioService,AudioControl,AudioPolicy(audio_policy_configuration.xml)三者相互配合,最终构成从usage到bus的映射关系.
这里需要说明的是,usage到context是多对一关系(多个usage可以对应一个context类型,但是反过来不行一个context不能对应多个usage),context到bus也是多对一关系,bus到output还是多对一关系


#====================================================================
15 Android Audio延迟(latency)
https://blog.csdn.net/iteye_17686/article/details/82339247
AUDIO latency
https://blog.csdn.net/weixin_33831673/article/details/93427873?utm_medium=distribute.pc_relevant_t0.none-task-blog-searchFromBaidu-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-searchFromBaidu-1.control

#====================================================================